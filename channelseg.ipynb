{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import model5\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(image, y_size, x_size, z_size):\n",
    "    image = image[0:x_size, 0:y_size, 0:z_size]\n",
    "    num_patches_per_x = image.shape[0] // 156\n",
    "    num_patches_per_y = image.shape[1] // 156\n",
    "    num_patches_per_z = image.shape[2] // 100\n",
    "    x_size_crop = num_patches_per_x * 156\n",
    "    y_size_crop = num_patches_per_y * 156\n",
    "    z_size_crop = num_patches_per_z * 100\n",
    "    image = image[0:x_size_crop, 0:y_size_crop, 0:z_size_crop]\n",
    "    patches = image.reshape(num_patches_per_x, 156,\n",
    "                            num_patches_per_y, 156, num_patches_per_z, 100)\n",
    "    patches = patches.transpose(0,2,4, 1,3,5)\n",
    "    patches = patches.reshape(num_patches_per_x * num_patches_per_y * num_patches_per_z,\n",
    "                              156, 156, 100)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_image(image, x_size, y_size, z_size):\n",
    "    image = image[0:x_size, 0:y_size, 0:z_size]\n",
    "    image1=image[:,62:217,:]\n",
    "    image2=image[:,0:156,:]\n",
    "    image3=image[:,124:280,:]\n",
    "    image4=image[:,:,284:440]\n",
    "    image1=image1.transpose(2,1,0)\n",
    "    image2=image2.transpose(2,1,0)\n",
    "    image3=image3.transpose(2,1,0)\n",
    "    image4=image4.transpose(2,1,0)\n",
    "    data1=parse_image(image1,156,440,100)\n",
    "    data2=parse_image(image2,156,440,100)\n",
    "    data3=parse_image(image3,156,440,100)\n",
    "    data4=parse_image(image4,280,156,100)\n",
    "    return data1,data2,data3,data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.zeros((3,4))\n",
    "a.shape\n",
    "a[1:3,1:4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 100, 156, 156)\n",
      "(375, 156, 156, 100)\n",
      "(375, 156, 156, 100)\n",
      "(375, 100, 156, 156)\n",
      "(375, 156, 156, 100)\n",
      "(375, 156, 156, 100)\n",
      "(375, 100, 156, 156)\n",
      "(375, 156, 156, 100)\n",
      "(375, 156, 156, 100)\n",
      "(1125, 156, 156, 100)\n",
      "(1125, 156, 156, 100)\n",
      "(312, 312, 100)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('chan.p', 'rb') as f:\n",
    "    dat = pickle.load(f, encoding='bytes') \n",
    "print(dat.shape)\n",
    "dat = dat.transpose(0,2,3,1)\n",
    "\n",
    "print(dat.shape)\n",
    "       \n",
    "    \n",
    "with open('chanlbl.p', 'rb') as f:\n",
    "    lbl = pickle.load(f, encoding='bytes')\n",
    "print(lbl.shape)\n",
    "with open('datpointy.p', 'rb') as f:\n",
    "    dat1 = pickle.load(f, encoding='bytes') \n",
    "print(dat1.shape)\n",
    "dat1 = dat1.transpose(0,2,3,1)\n",
    "\n",
    "print(dat1.shape)\n",
    "       \n",
    "    \n",
    "with open('labelpointy.p', 'rb') as f:\n",
    "    label1 = pickle.load(f, encoding='bytes')\n",
    "print(label1.shape)\n",
    "with open('datmudn.p', 'rb') as f:\n",
    "    dat2 = pickle.load(f, encoding='bytes') \n",
    "print(dat2.shape)\n",
    "dat2 = dat2.transpose(0,2,3,1)\n",
    "\n",
    "print(dat2.shape)\n",
    "       \n",
    "    \n",
    "with open('labelmudn.p', 'rb') as f:\n",
    "    label2 = pickle.load(f, encoding='bytes')\n",
    "print(label2.shape)\n",
    "\n",
    "data1=np.concatenate((dat,dat1,dat2),axis=0)\n",
    "label=np.concatenate((lbl,label1,label2),axis=0)\n",
    "data=(data1-np.mean(data1))/np.std(data1)\n",
    "\n",
    "print(data.shape)\n",
    "print(label.shape)\n",
    "\n",
    "\n",
    "training_data,eval_data,training_label,eval_label=train_test_split(data,label,test_size=0.005,random_state=42)\n",
    "with open('dw.p', 'rb') as f:\n",
    "    test = pickle.load(f, encoding='bytes') \n",
    "test=(test-np.mean(data1))/np.std(data1)\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1119, 156, 156, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_label.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1119, 156, 156, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 156, 156, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=parse_image(test, 312, 312, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 4):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "   \n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:,:,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(math.floor(m/mini_batch_size)) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 156, 156, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist(predictions, labels):\n",
    "    num_class = predictions.shape[4]\n",
    "    batch_size = predictions.shape[0]\n",
    "    hist = np.zeros((num_class, num_class))\n",
    "    for i in range(batch_size):\n",
    "        hist += fast_hist(labels[i].flatten(), predictions[i].argmax(3).flatten(), num_class)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAX_VOTE(pred,prob,NUM_CLASS):\n",
    "    \"\"\"\n",
    "    logit: the shape should be [NUM_SAMPLES,Batch_size, image_h,image_w,NUM_CLASS]\n",
    "    pred: the shape should be[NUM_SAMPLES,NUM_PIXELS]\n",
    "    label: the real label information for each image\n",
    "    prob: the probability, the shape should be [NUM_SAMPLES,image_h,image_w,NUM_CLASS]\n",
    "    Output:\n",
    "    logit: which will feed into the Normal loss function to calculate loss and also accuracy!\n",
    "    \"\"\"\n",
    "\n",
    "    image_h = 156\n",
    "    image_w = 156\n",
    "    image_d = 100\n",
    "    NUM_SAMPLES = np.shape(pred)[0]\n",
    "    #transpose the prediction to be [NUM_PIXELS,NUM_SAMPLES]\n",
    "    pred_tot = np.transpose(pred)\n",
    "    prob_re = np.reshape(prob,[NUM_SAMPLES,image_h*image_w*image_d,NUM_CLASS])\n",
    "    prediction = []\n",
    "    variance_final = []\n",
    "    step = 0\n",
    "    for i in pred_tot:\n",
    "        \n",
    "        value = np.bincount(i,minlength = NUM_CLASS)\n",
    "        value_max = np.argmax(value)\n",
    "        #indices = [k for k,j in enumerate(i) if j == value_max]\n",
    "        indices = np.where(i == value_max)[0]\n",
    "        prediction.append(value_max)\n",
    "        variance_final.append(np.var(prob_re[indices,step,:],axis = 0))\n",
    "        step = step+1\n",
    "        \n",
    "     \n",
    "    return variance_final,prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(photo, batch_size):\n",
    "    \n",
    "    while (True):\n",
    "        for i in range(0,batch_size):\n",
    "            yield(photo[i:i+batch_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Convnet1/convonet1/Relu:0\", shape=(?, 156, 156, 100, 16), dtype=float32)\n",
      "Tensor(\"Maxpool1/mpool1:0\", shape=(?, 78, 78, 50, 16), dtype=float32)\n",
      "Tensor(\"Convnet2/convonet2/Relu:0\", shape=(?, 78, 78, 50, 16), dtype=float32)\n",
      "Tensor(\"Maxpool2/mpool2:0\", shape=(?, 39, 39, 25, 16), dtype=float32)\n",
      "Tensor(\"Convnet3/convonet3/Relu:0\", shape=(?, 39, 39, 25, 16), dtype=float32)\n",
      "Tensor(\"Maxpool3/mpool3:0\", shape=(?, 20, 20, 13, 16), dtype=float32)\n",
      "Tensor(\"Convnet4/convonet4/Relu:0\", shape=(?, 20, 20, 13, 16), dtype=float32)\n",
      "Tensor(\"Maxpool4/mpool4:0\", shape=(?, 10, 10, 7, 16), dtype=float32)\n",
      "Tensor(\"Deconv4/upsamp4/conv3d_transpose:0\", shape=(?, 20, 20, 13, 16), dtype=float32)\n",
      "Tensor(\"Deconv4/conv_decode4/cond/Merge:0\", shape=(?, 20, 20, 13, 16), dtype=float32)\n",
      "Tensor(\"Deconv3/upsamp3/conv3d_transpose:0\", shape=(?, 39, 39, 25, 16), dtype=float32)\n",
      "Tensor(\"Deconv3/conv_decode3/cond/Merge:0\", shape=(?, 39, 39, 25, 16), dtype=float32)\n",
      "Tensor(\"Deconv2/upsamp2/conv3d_transpose:0\", shape=(?, 78, 78, 50, 16), dtype=float32)\n",
      "Tensor(\"Deconv2/conv_decode2/cond/Merge:0\", shape=(?, 78, 78, 50, 16), dtype=float32)\n",
      "Tensor(\"Deconv1/upsamp1/conv3d_transpose:0\", shape=(?, 156, 156, 100, 16), dtype=float32)\n",
      "Tensor(\"Deconv1/conv_decode1/cond/Merge:0\", shape=(?, 156, 156, 100, 16), dtype=float32)\n",
      "Tensor(\"Classifier/Conv3D:0\", shape=(?, 156, 156, 100, 2), dtype=float32)\n",
      "Tensor(\"Classifier/BiasAdd:0\", shape=(?, 156, 156, 100, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "with tf.name_scope(\"Input\"):\n",
    "    x = tf.placeholder(tf.float32, [None,156,156,100])\n",
    "    y = tf.placeholder(tf.int32, [None,156,156,100])\n",
    "    keepprob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "loss, logits = model5.inference(x,y, keepprob, is_training)\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "\n",
    "pred = tf.cast(tf.argmax(tf.nn.softmax(logits), -1), tf.int32)\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Train\"):\n",
    "    grad_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = grad_op.minimize(loss, global_step=global_step)\n",
    "\n",
    "tf.summary.scalar(\"cost\", loss)\n",
    "tf.summary.histogram('histogram_loss', loss)\n",
    "summary_op = tf.summary.merge_all()\n",
    "import os\n",
    "try:\n",
    "    os.mkdir('checkpoints')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir('checkpoints/channel_01')\n",
    "except OSError:\n",
    "    pass\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plots_bayes(images, labels, predicted_labels, uncertainty):\n",
    "    \n",
    "    num_images = len(images)\n",
    "    \n",
    "    cols = ['Input', 'Ground truth', 'Output', 'Uncertainty']\n",
    "    rows = ['Image {}'.format(row) for row in range(1,num_images+1)]\n",
    "    #rows = ['Worst', 'Average', 'Best']\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_images, ncols=4, figsize=(20,num_images*4))\n",
    "    \n",
    "\n",
    "    for i in range(num_images):\n",
    "\n",
    "        plt.subplot(num_images, 4, (4*i+1))\n",
    "        plt.imshow(images[i,:,:,50])\n",
    "        #plt.ylabel(\"Image %d\" % (i+1), size='18')\n",
    "        plt.ylabel(rows[i], size='22')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        if (i==0): \n",
    "            plt.title(cols[0], size='22', va='bottom')\n",
    "\n",
    "        plt.subplot(num_images, 4, (4*i+2))\n",
    "        plt.imshow(labels[i,:,:,50])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        if (i==0): \n",
    "            plt.title(cols[1], size='22', va='bottom')\n",
    "\n",
    "        plt.subplot(num_images, 4, (4*i+3))\n",
    "        plt.imshow(predicted_labels[i,:,:,50])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        if (i==0): \n",
    "            plt.title(cols[2], size='22', va='bottom')\n",
    "            \n",
    "        plt.subplot(num_images, 4, (4*i+4))\n",
    "        plt.imshow(uncertainty[i,:,:,50], cmap = 'Greys')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        if (i==0): \n",
    "            plt.title(cols[3], size='22', va='bottom')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plots_bayes_external(images, predicted_labels, uncertainty):\n",
    "    \n",
    "    num_images = images.shape[0]\n",
    "    \n",
    "    cols = ['Input', 'Output', 'Uncertainty']\n",
    "    rows = ['Image {}'.format(row) for row in range(1,num_images+1)]\n",
    "    #rows = ['Worst', 'Average', 'Best']\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_images, ncols=3, figsize=(16,num_images*4))\n",
    "    \n",
    "\n",
    "    for i in range(num_images):\n",
    "\n",
    "        plt.subplot(num_images, 3, (3*i+1))\n",
    "        plt.imshow(images[i,:,:,50])\n",
    "        #plt.ylabel(\"Image %d\" % (i+1), size='18')\n",
    "        plt.ylabel(rows[i], size='18')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        if (i==0): \n",
    "            plt.title(cols[0], size='18', va='bottom')\n",
    "\n",
    "        plt.subplot(num_images, 3, (3*i+2))\n",
    "        plt.imshow(predicted_labels[i,:,:,50])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        if (i==0): \n",
    "            plt.title(cols[1], size='18', va='bottom')\n",
    "            \n",
    "        plt.subplot(num_images, 3, (3*i+3))\n",
    "        plt.imshow(uncertainty[i,:,:,50], cmap = 'Greys')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        if (i==0): \n",
    "            plt.title(cols[2], size='18', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_calculate(pred,prob_variance):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    pred: predicted label, shape is [NUM_PIXEL,1]\n",
    "    prob_variance: the total variance for 12 classes wrt each pixel, prob_variance shape [image_h,image_w,2]\n",
    "    Output:\n",
    "    var_one: corresponding variance in terms of the \"optimal\" label\n",
    "    \"\"\"\n",
    "        \n",
    "    image_h = 156\n",
    "    image_w = 156\n",
    "    image_c = 100\n",
    "    NUM_CLASS = np.shape(prob_variance)[-1]\n",
    "    var_sep = [] #var_sep is the corresponding variance if this pixel choose label k\n",
    "    length_cur = 0 #length_cur represent how many pixels has been read for one images\n",
    "    for row in np.reshape(prob_variance,[image_h*image_w*image_c,NUM_CLASS]):\n",
    "        temp = row[pred[length_cur]]\n",
    "        length_cur += 1\n",
    "        var_sep.append(temp)\n",
    "    var_one = np.reshape(var_sep,[image_h,image_w, image_c]) #var_one is the corresponding variance in terms of the \"optimal\" label\n",
    "    \n",
    "    return var_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/channel_01/channel-convnet-10404\n"
     ]
    }
   ],
   "source": [
    "begin_time = time.time()\n",
    "num_epochs = 4\n",
    "batch_size = 7\n",
    "costs = []\n",
    "accs=[]\n",
    "import os\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth= True\n",
    "sess = tf.Session(config=config)\n",
    "init_op.run(session=sess)\n",
    "# to visualize using TensorBoard\n",
    "writer = tf.summary.FileWriter('./graphs/channel_01', sess.graph)\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/channel_01/checkpoint'))\n",
    "# if that checkpoint exists, restore from checkpoint\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "output_list = None\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    minibatch_cost = 0.\n",
    "    num_minibatches = int(training_data.shape[0] / batch_size) # number of minibatches of size minibatch_size in the train set\n",
    "    \n",
    "    minibatches = random_mini_batches(training_data, training_label, batch_size)\n",
    "    \n",
    "    count = 0\n",
    "    i=0\n",
    "    histo = np.zeros((2, 2))\n",
    "    \n",
    "    for minibatch in minibatches:\n",
    "\n",
    "        # Select a minibatch\n",
    "        (minibatch_X, minibatch_Y) = minibatch\n",
    "        \n",
    "        _, step, output, cost,summary = sess.run(\n",
    "            [train_op, global_step, tf.nn.softmax(logits), loss,summary_op],\n",
    "            feed_dict={x: minibatch_X, y: minibatch_Y, keepprob:0.7, is_training:True})\n",
    "        count += 1\n",
    "        \n",
    "        hist = get_hist(output, minibatch_Y)\n",
    "        acc = np.diag(hist).sum() / hist.sum()\n",
    "        \n",
    "            \n",
    "        if count % 50 == 0 or i+1 == num_minibatches:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            print(\"Step: %d, \" % (step+1)+\n",
    "                  \"  Epoch: %2d,\" % (epoch+1)+\n",
    "                  \"  Batch: %3d of %3d,\" % (i+1, num_minibatches)+\n",
    "                  \"  Cost: %.4f,\" % cost+\n",
    "                  \"  Accuracy: %.4f,\" % acc+\n",
    "                  \" AvgTime/Sample: %3.2fms\" % float(elapsed_time*1000/count/4))\n",
    "            saver.save(sess, 'checkpoints/channel_01/channel-convnet', step)                  \n",
    "            \n",
    "            count = 0\n",
    "        i+=1\n",
    "            \n",
    "        minibatch_cost += cost / num_minibatches\n",
    "        histo += hist\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    acc_total = np.diag(histo).sum() / histo.sum()\n",
    "    iu = np.diag(histo) / (histo.sum(1) + histo.sum(0) - np.diag(histo))\n",
    "    if epoch%1==0:\n",
    "        print (\"Cost after epoch %i: %f\" % (epoch+1, minibatch_cost))\n",
    "        print(\"acc: \", acc_total)\n",
    "        print(\"mean IU: \", np.nanmean(iu))\n",
    "    if epoch % 1 == 0:\n",
    "        costs.append(minibatch_cost)\n",
    "        accs.append(acc_total)\n",
    "    \n",
    "        \n",
    "# plot the cost\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n",
    "print(\"Total Time: %3.2fs\" % float(time.time() - begin_time))\n",
    "\n",
    "#draw_plots_bayes(training_data[0:4,:,:,:], training_label[0:4,:,:,:], pred_tot[0:4,:,:,:], var_tot[0:4,:,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cost\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n",
    "print(\"Total Time: %3.2fs\" % float(time.time() - begin_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "figsize(50,50)\n",
    "\n",
    "pred = np.argmax(output, axis=-1)\n",
    "\n",
    "for select in range(1):\n",
    "    plt.subplot(1,3, select*3+1)\n",
    "    plt.imshow(np.transpose(minibatch_X[select, 100,:,:]))\n",
    "    plt.subplot(1,3, select*3+2)\n",
    "    plt.imshow(np.transpose(minibatch_Y[select, 100,:,:]))\n",
    "    plt.subplot(1,3, select*3+3)\n",
    "    plt.imshow(np.transpose(output[select, 100,:,:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_images = np.zeros([1, 156, 156, 100])\n",
    "eval_lbl = np.zeros([1, 156, 156, 100])\n",
    "null_labels = np.zeros_like(eval_images).astype(np.int32)\n",
    "output_eval=None\n",
    "\n",
    "for i in range(1):\n",
    "    eval_images[i,:,:,:]=eval_data[i,:,:,:]\n",
    "    eval_lbl[i,:,:,:]=eval_label[i,:,:,:]\n",
    "\n",
    "num_sample_generate = 30\n",
    "pred_tot = []\n",
    "var_tot = []    \n",
    "\n",
    "eval_output = sess.run(tf.nn.softmax(logits), feed_dict={x: eval_images, y: null_labels, keepprob:1, is_training:True})\n",
    "if output_eval is not None:\n",
    "    output_eval = np.concatenate((output_eval, eval_output), axis=0)\n",
    "else:\n",
    "    output_eval = eval_output\n",
    "\n",
    "figsize(50,50)\n",
    "for select in range(1):\n",
    "    plt.subplot(4,3, select*3+1)\n",
    "    plt.imshow(np.transpose(eval_images[select, :,:,69]))\n",
    "    plt.subplot(4,3, select*3+2)\n",
    "    plt.imshow(np.transpose(output_eval[select, :,:,69, 1]))\n",
    "    plt.subplot(4,3, select*3+3)\n",
    "    plt.imshow(np.transpose(eval_lbl[select, :,:,69]))\n",
    "\n",
    "\n",
    "for image_batch, label_batch in zip(eval_images,eval_lbl):\n",
    "    image_batch = np.reshape(image_batch,[1,156,156,100])\n",
    "    label_batch = np.reshape(label_batch,[1,156,156,100])\n",
    "    prob_iter_tot = []\n",
    "    pred_iter_tot = []\n",
    "    \n",
    "    for iter_step in range(num_sample_generate):\n",
    "        prob_iter_step = sess.run(tf.nn.softmax(logits), feed_dict = {x: image_batch, y: label_batch, keepprob:0.7, is_training:True}) \n",
    "        prob_iter_tot.append(prob_iter_step)\n",
    "        pred_iter_tot.append(np.reshape(np.argmax(prob_iter_step,axis = -1),[-1]))\n",
    "        \n",
    "    prob_variance,pred=MAX_VOTE(pred_iter_tot,prob_iter_tot,2)\n",
    "    acc_per=np.mean(np.equal(pred,np.reshape(label_batch,[-1])))\n",
    "    var_one=var_calculate(pred,prob_variance)\n",
    "    pred=np.reshape(pred,[156,156,100])\n",
    "    print(\"Accuracy after 30 samples: %f\" % (acc_per))\n",
    "    #prob_mean = np.nanmean(prob_iter_tot,axis = 0)\n",
    "    #prob_variance = np.var(prob_iter_tot, axis = 0)\n",
    "    #pred = np.reshape(np.argmax(prob_mean,axis = -1),[-1]) #pred is the predicted label with the mean of generated samples\n",
    "    #THIS TIME I DIDN'T INCLUDE TAU\n",
    "    #var_one = var_calculate(pred,prob_variance)\n",
    "    #pred = np.reshape(pred,[156,156,100])\n",
    "    pred_tot.append(pred)\n",
    "    var_tot.append(var_one)\n",
    "pred_tot=np.array(pred_tot)\n",
    "var_tot=np.array(var_tot)\n",
    "draw_plots_bayes(eval_images, eval_lbl, pred_tot, var_tot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_images = np.zeros([4, 156, 156, 100])\n",
    "\n",
    "null_labels_test = np.zeros_like(test_images).astype(np.int32)\n",
    "output_test=None\n",
    "for i in range(4):\n",
    "    test_images[i,:,:,:]=test_data[i,:,:,:]\n",
    "\n",
    "num_sample_generate = 300\n",
    "pred_tot = []\n",
    "var_tot = []\n",
    "prob_tot=[]\n",
    "\n",
    "test_output = sess.run(tf.nn.softmax(logits), feed_dict={x: test_images, y: null_labels_test, keepprob:1, is_training:True})\n",
    "if output_test is not None:\n",
    "    output_test = np.concatenate((output_test, test_output), axis=0)\n",
    "else:\n",
    "    output_test = test_output\n",
    "\n",
    "figsize(50,50)\n",
    "for select in range(4):\n",
    "    plt.subplot(2,4, select*2+1)\n",
    "    plt.imshow(test_images[select, :,:,70])\n",
    "    plt.subplot(2,4, select*2+2)\n",
    "    plt.imshow(output_test[select,:,:,70, 1])\n",
    "\n",
    "for image_batch, label_batch in zip(test_images,null_labels_test):\n",
    "    image_batch = np.reshape(image_batch,[1,156,156,100])\n",
    "    label_batch = np.reshape(label_batch,[1,156,156,100])\n",
    "    prob_iter_tot = []\n",
    "    pred_iter_tot = []\n",
    "    \n",
    "    for iter_step in range(num_sample_generate):\n",
    "        prob_iter_step = sess.run(tf.nn.softmax(logits), feed_dict = {x: image_batch, y: label_batch, keepprob:0.7, is_training:True}) \n",
    "        prob_iter_tot.append(prob_iter_step)\n",
    "        pred_iter_tot.append(np.reshape(np.argmax(prob_iter_step,axis = -1),[-1]))\n",
    "\n",
    "    #prob_variance,pred=MAX_VOTE(pred_iter_tot,prob_iter_tot,2)\n",
    "    #acc_per=np.mean(np.equal(pred,np.reshape(label_batch,[-1])))\n",
    "    #var_one=var_calculate(pred,prob_variance)\n",
    "    #pred=np.reshape(pred,[156,156,100])\n",
    "    #print(\"Accuracy after 10 samples: %f\" % (acc_per))\n",
    "    \n",
    "    prob_mean = np.nanmean(prob_iter_tot,axis = 0)\n",
    "    prob_variance = np.var(prob_iter_tot, axis = 0)\n",
    "    \n",
    "    pred = np.reshape(np.argmax(prob_mean,axis = -1),[-1]) #pred is the predicted label with the mean of generated samples\n",
    "    #THIS TIME I DIDN'T INCLUDE TAU\n",
    "    var_one = var_calculate(pred,prob_variance)\n",
    "    pred = np.reshape(pred,[156,156,100])\n",
    "    prob=prob_mean[0,:,:,:,1]\n",
    "    prob_variance=np.reshape(prob_variance,[156*156*100,2])\n",
    "    variance=np.nanmean(prob_variance,axis=1)\n",
    "    variance=np.reshape(variance,(156,156,100))\n",
    "    pred_tot.append(prob)\n",
    "    prob_tot.append(pred)\n",
    "    var_tot.append(var_one)\n",
    "pred_tot=np.array(pred_tot)\n",
    "print(pred_tot.shape)\n",
    "var_tot=np.array(var_tot)\n",
    "draw_plots_bayes_external(test_images, pred_tot, var_tot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_tot=np.array(prob_tot)\n",
    "prob_tot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=np.concatenate((np.concatenate((pred_tot[0,:,:,:],pred_tot[1,:,:,:]),axis=1),np.concatenate((pred_tot[2,:,:,:],pred_tot[3,:,:,:]),axis=1)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=np.concatenate((np.concatenate((var_tot[0,:,:,:],var_tot[1,:,:,:]),axis=1),np.concatenate((var_tot[2,:,:,:],var_tot[3,:,:,:]),axis=1)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=np.concatenate((np.concatenate((output_test[0,:,:,:,1],output_test[1,:,:,:,1]),axis=1),np.concatenate((output_test[2,:,:,:,1],output_test[3,:,:,:,1]),axis=1)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out.tofile(\"channel156_01_6_var\",format=\"%4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
